MTI1 = ['SpaceInvaders-v0', 'CrazyClimber-v0', 'Seaquest-v0', 'DemonAttack-v0', 'StarGunner-v0']
#actionspace disc 6             disc 9              disc 18         disc 6          disc 18
MTI2 = ['SpaceInvaders-v0', 'Seaquest-v0']

target_performances = {
    'SpaceInvaders-v0': 1200,
    'Seaquest-v0': 2700,
    'Asterix-v0': 2400,
    'Alien-v0': 2700,
    'Assault-v0': 1900,
    'TimePilot-v0': 9000,
    'BankHeist-v0': 1700,
    'CrazyClimber-v0': 170000,
    'DemonAttack-v0': 27000,
    'Gropher-v0': 9400, #?
    'NameThisGame-v0': 12100,
    'StarGunner-v0': 40000,
    'Tutankham-v0': 260,
    'Amidar-v0': 1030,
    'ChopperCommand-v0': 4970,
    'Breakout-v0': 560,
    'BeamRider-v0': 2200,
    'Bowling-v0': 17,
    'Centipede-v0': 3300,
    'Krull-v0': 1025,
    'Kangaroo-v0': 26,
    'Phoenix-v0': 5384,
    'Atlantis-v0': 163660,
    'Frostbite-v0': 300,
    'KungFuMaster-v0': 36000,
    'Pond-v0': 19.5,
    'RoadRunner-v0': 59540,
    'Qbert-v0': 26000,
    'WizardofWor-v0': 3300,
    'Enduro-v0': 0.77
}

l = 100_000  # Number of training steps for which a uniformly random policy is executed for task selection. At the end of l training steps, the agent must have learned on â‰¥ n
MaxSteps = 1_000_000
tau = 1

model_path = ""
